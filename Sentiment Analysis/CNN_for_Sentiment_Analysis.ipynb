{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    CNN implementation\n",
    "    author: @abdulsmapara\n",
    "'''\n",
    "\n",
    "# import statements\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DEFINE CNN\n",
    "'''\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                           SENTENCE LABEL  POS  \\\n",
      "0           0  BBC News: Labour MPs pass Corbyn no-confidence...   POS    1   \n",
      "1           1  2,500 Scientists To Australia: If You Want To ...   POS    1   \n",
      "2           2  Today The United Kingdom decides whether to re...   NEU    0   \n",
      "3           3  Canadian Rescue Plane successfully reaches Sou...   POS    1   \n",
      "\n",
      "   NEG  NEU                                         Text_final  \\\n",
      "0    0    0  bbc news labour mps pass corbyn noconfidence m...   \n",
      "1    0    0  2500 scientists australia want save great barr...   \n",
      "2    0    1  today united kingdom decides remain european u...   \n",
      "3    0    0  canadian rescue plane successfully reaches sou...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [bbc, news, labour, mps, pass, corbyn, noconfi...  \n",
      "1  [2500, scientists, australia, want, save, grea...  \n",
      "2  [today, united, kingdom, decides, remain, euro...  \n",
      "3  [canadian, rescue, plane, successfully, reache...  \n",
      "19504 words total, with a vocabulary size of 6982\n",
      "Max sentence length is 37\n",
      "2119 words total, with a vocabulary size of 1511\n",
      "Max sentence length is 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdulsmapara/environments/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:76: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/home/abdulsmapara/environments/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:76: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 40, 300)      2089500     input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, 39, 200)      120200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, 38, 200)      180200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, 37, 200)      240200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, 36, 200)      300200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_70 (Conv1D)              (None, 35, 200)      360200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_66 (Global (None, 200)          0           conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_67 (Global (None, 200)          0           conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_68 (Global (None, 200)          0           conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_69 (Global (None, 200)          0           conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_70 (Global (None, 200)          0           conv1d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 1000)         0           global_max_pooling1d_66[0][0]    \n",
      "                                                                 global_max_pooling1d_67[0][0]    \n",
      "                                                                 global_max_pooling1d_68[0][0]    \n",
      "                                                                 global_max_pooling1d_69[0][0]    \n",
      "                                                                 global_max_pooling1d_70[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 1000)         0           concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 128)          128128      dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 128)          0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 3)            387         dropout_28[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,419,015\n",
      "Trainable params: 1,329,515\n",
      "Non-trainable params: 2,089,500\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "1968/1968 [==============================] - 14s 7ms/step - loss: 0.4823 - acc: 0.7700\n",
      "Epoch 2/15\n",
      "1968/1968 [==============================] - 5s 2ms/step - loss: 0.3552 - acc: 0.8379\n",
      "Epoch 3/15\n",
      "1968/1968 [==============================] - 6s 3ms/step - loss: 0.2638 - acc: 0.8908\n",
      "Epoch 4/15\n",
      "1968/1968 [==============================] - 5s 3ms/step - loss: 0.1672 - acc: 0.9431\n",
      "Epoch 5/15\n",
      "1968/1968 [==============================] - 5s 2ms/step - loss: 0.1005 - acc: 0.9746\n",
      "Epoch 6/15\n",
      "1968/1968 [==============================] - 4s 2ms/step - loss: 0.0549 - acc: 0.9876\n",
      "Epoch 7/15\n",
      "1968/1968 [==============================] - 5s 2ms/step - loss: 0.0378 - acc: 0.9920\n",
      "Epoch 8/15\n",
      "1968/1968 [==============================] - 5s 3ms/step - loss: 0.0243 - acc: 0.9961\n",
      "Epoch 9/15\n",
      "1968/1968 [==============================] - 5s 3ms/step - loss: 0.0157 - acc: 0.9968\n",
      "Epoch 10/15\n",
      "1968/1968 [==============================] - 7s 3ms/step - loss: 0.0123 - acc: 0.9978\n",
      "Epoch 11/15\n",
      "1968/1968 [==============================] - 6s 3ms/step - loss: 0.0141 - acc: 0.9986\n",
      "Epoch 12/15\n",
      "1968/1968 [==============================] - 6s 3ms/step - loss: 0.0100 - acc: 0.9985\n",
      "Epoch 13/15\n",
      "1968/1968 [==============================] - 6s 3ms/step - loss: 0.0062 - acc: 0.9988\n",
      "Epoch 14/15\n",
      "1968/1968 [==============================] - 6s 3ms/step - loss: 0.0048 - acc: 0.9990\n",
      "Epoch 15/15\n",
      "1968/1968 [==============================] - 7s 3ms/step - loss: 0.0055 - acc: 0.9992\n",
      "219/219 [==============================] - 1s 3ms/step\n",
      "ACCURACY:  0.7808219178082192\n",
      "NEU    134\n",
      "NEG     54\n",
      "POS     31\n",
      "Name: LABEL, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    text = nlp(text_nopunct.lower())\n",
    "    final_text = \"\"\n",
    "    for token in text:\n",
    "        if token.is_stop == False:\n",
    "            final_text += token.text + \" \"\n",
    "    return final_text.strip()\n",
    "\n",
    "def get_tokens(text):\n",
    "    text = nlp(text)\n",
    "    tokens = []\n",
    "    for token in text:\n",
    "        tokens.append(token.text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load('custom_word2vec.model')\n",
    "data = pd.read_csv('labelled_news.csv')\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "neu = []\n",
    "for l in data.LABEL:\n",
    "    if l == \"POS\":\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "        neu.append(0)\n",
    "    elif l == \"NEG\":\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "        neu.append(0)\n",
    "    else:\n",
    "        pos.append(0)\n",
    "        neg.append(0)\n",
    "        neu.append(1)\n",
    "data['POS'] = pos\n",
    "data['NEG'] = neg\n",
    "data['NEU'] = neu\n",
    "\n",
    "\n",
    "data['Text_final'] = data['SENTENCE'].apply(lambda x: preprocessing(x))\n",
    "data['tokens'] = data['Text_final'].apply(lambda x: get_tokens(x))\n",
    "print(data[:4])\n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)\n",
    "\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "\n",
    "\n",
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Text_final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_final\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.zeros(EMBEDDING_DIM)\n",
    "# print(train_embedding_weights.shape)\n",
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "label_names = ['POS', 'NEG', 'NEU']\n",
    "y_train = data_train[label_names].values\n",
    "x_train = train_cnn_data\n",
    "y_tr = y_train\n",
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))\n",
    "num_epochs = 15\n",
    "batch_size = 64\n",
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0, shuffle=True, batch_size=batch_size)\n",
    "predictions = model.predict(test_cnn_data, batch_size=64, verbose=1)\n",
    "\n",
    "labels = ['POS','NEG','NEU']\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "print(\"ACCURACY: \", sum(data_test.LABEL==prediction_labels)/len(prediction_labels))\n",
    "print(data_test.LABEL.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
