{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    CNN implementation\n",
    "    author: @abdulsmapara\n",
    "'''\n",
    "\n",
    "# import statements\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DEFINE CNN\n",
    "'''\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    text = nlp(text_nopunct.lower())\n",
    "    final_text = \"\"\n",
    "    for token in text:\n",
    "        if token.is_stop == False:\n",
    "            final_text += token.text + \" \"\n",
    "    return final_text.strip()\n",
    "\n",
    "def get_tokens(text):\n",
    "    text = nlp(text)\n",
    "    tokens = []\n",
    "    for token in text:\n",
    "        tokens.append(token.text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load('custom_word2vec.model')\n",
    "data = pd.read_csv('labelled_news.csv')\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "neu = []\n",
    "for l in data.LABEL:\n",
    "    if l == \"POS\":\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "        neu.append(0)\n",
    "    elif l == \"NEG\":\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "        neu.append(0)\n",
    "    else:\n",
    "        pos.append(0)\n",
    "        neg.append(0)\n",
    "        neu.append(1)\n",
    "data['POS'] = pos\n",
    "data['NEG'] = neg\n",
    "data['NEU'] = neu\n",
    "\n",
    "\n",
    "data['Text_final'] = data['SENTENCE'].apply(lambda x: preprocessing(x))\n",
    "data['tokens'] = data['Text_final'].apply(lambda x: get_tokens(x))\n",
    "print(data[:4])\n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size=0.20, random_state=42)\n",
    "\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "\n",
    "\n",
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Text_final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_final\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "# print(train_embedding_weights.shape)\n",
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "label_names = ['POS', 'NEG', 'NEU']\n",
    "y_train = data_train[label_names].values\n",
    "x_train = train_cnn_data\n",
    "y_tr = y_train\n",
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))\n",
    "num_epochs = 8\n",
    "batch_size = 64\n",
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.03, shuffle=True, batch_size=batch_size)\n",
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "\n",
    "labels = ['POS','NEG','NEU']\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "print(\"ACCURACY: \", sum(data_test.LABEL==prediction_labels)/len(prediction_labels))\n",
    "print(data_test.LABEL.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
