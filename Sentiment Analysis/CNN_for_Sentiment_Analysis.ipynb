{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    CNN implementation\n",
    "    author: @abdulsmapara\n",
    "'''\n",
    "\n",
    "# import statements\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DEFINE CNN\n",
    "'''\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                           SENTENCE LABEL  POS  \\\n",
      "0           0  BBC News: Labour MPs pass Corbyn no-confidence...   POS    1   \n",
      "1           1  2,500 Scientists To Australia: If You Want To ...   POS    1   \n",
      "2           2  Today The United Kingdom decides whether to re...   NEU    0   \n",
      "3           3  Canadian Rescue Plane successfully reaches Sou...   POS    1   \n",
      "\n",
      "   NEG  NEU                                         Text_final  \\\n",
      "0    0    0  bbc news labour mps pass corbyn noconfidence m...   \n",
      "1    0    0  2500 scientists australia want save great barr...   \n",
      "2    0    1  today united kingdom decides remain european u...   \n",
      "3    0    0  canadian rescue plane successfully reaches sou...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [bbc, news, labour, mps, pass, corbyn, noconfi...  \n",
      "1  [2500, scientists, australia, want, save, grea...  \n",
      "2  [today, united, kingdom, decides, remain, euro...  \n",
      "3  [canadian, rescue, plane, successfully, reache...  \n",
      "19324 words total, with a vocabulary size of 6980\n",
      "Max sentence length is 37\n",
      "2157 words total, with a vocabulary size of 1512\n",
      "Max sentence length is 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdulsmapara/environments/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:76: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/home/abdulsmapara/environments/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:76: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 40, 300)      2088900     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_81 (Conv1D)              (None, 39, 200)      120200      embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_82 (Conv1D)              (None, 38, 200)      180200      embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_83 (Conv1D)              (None, 37, 200)      240200      embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_84 (Conv1D)              (None, 36, 200)      300200      embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_85 (Conv1D)              (None, 35, 200)      360200      embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_81 (Global (None, 200)          0           conv1d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_82 (Global (None, 200)          0           conv1d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_83 (Global (None, 200)          0           conv1d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_84 (Global (None, 200)          0           conv1d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_85 (Global (None, 200)          0           conv1d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 1000)         0           global_max_pooling1d_81[0][0]    \n",
      "                                                                 global_max_pooling1d_82[0][0]    \n",
      "                                                                 global_max_pooling1d_83[0][0]    \n",
      "                                                                 global_max_pooling1d_84[0][0]    \n",
      "                                                                 global_max_pooling1d_85[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 1000)         0           concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 128)          128128      dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 128)          0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 3)            387         dropout_34[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,418,415\n",
      "Trainable params: 1,329,515\n",
      "Non-trainable params: 2,088,900\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "1948/1948 [==============================] - 6s 3ms/step - loss: 0.4677 - acc: 0.7752\n",
      "Epoch 2/15\n",
      "1948/1948 [==============================] - 5s 3ms/step - loss: 0.3314 - acc: 0.8510\n",
      "Epoch 3/15\n",
      "1948/1948 [==============================] - 5s 3ms/step - loss: 0.2382 - acc: 0.9042\n",
      "Epoch 4/15\n",
      "1948/1948 [==============================] - 5s 2ms/step - loss: 0.1331 - acc: 0.9605\n",
      "Epoch 5/15\n",
      "1948/1948 [==============================] - 5s 2ms/step - loss: 0.0715 - acc: 0.9812\n",
      "Epoch 6/15\n",
      "1948/1948 [==============================] - 5s 2ms/step - loss: 0.0424 - acc: 0.9921\n",
      "Epoch 7/15\n",
      "1948/1948 [==============================] - 5s 2ms/step - loss: 0.0279 - acc: 0.9947\n",
      "Epoch 8/15\n",
      "1948/1948 [==============================] - 5s 2ms/step - loss: 0.0181 - acc: 0.9974\n",
      "Epoch 9/15\n",
      "1948/1948 [==============================] - 5s 2ms/step - loss: 0.0164 - acc: 0.9979\n",
      "Epoch 10/15\n",
      "1948/1948 [==============================] - 5s 2ms/step - loss: 0.0079 - acc: 0.9990\n",
      "Epoch 11/15\n",
      "1948/1948 [==============================] - 5s 2ms/step - loss: 0.0056 - acc: 0.9993\n",
      "Epoch 12/15\n",
      "1948/1948 [==============================] - 5s 2ms/step - loss: 0.0043 - acc: 0.9995\n",
      "Epoch 13/15\n",
      "1948/1948 [==============================] - 5s 2ms/step - loss: 0.0040 - acc: 0.9995\n",
      "Epoch 14/15\n",
      "1948/1948 [==============================] - 5s 2ms/step - loss: 0.0033 - acc: 0.9995\n",
      "Epoch 15/15\n",
      "1948/1948 [==============================] - 5s 2ms/step - loss: 0.0027 - acc: 0.9995\n",
      "217/217 [==============================] - 0s 2ms/step\n",
      "ACCURACY:  0.7465437788018433\n",
      "NEU    133\n",
      "NEG     49\n",
      "POS     35\n",
      "Name: LABEL, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    text = nlp(text_nopunct.lower())\n",
    "    final_text = \"\"\n",
    "    for token in text:\n",
    "        if token.is_stop == False:\n",
    "            final_text += token.text + \" \"\n",
    "    return final_text.strip()\n",
    "\n",
    "def get_tokens(text):\n",
    "    text = nlp(text)\n",
    "    tokens = []\n",
    "    for token in text:\n",
    "        tokens.append(token.text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load('custom_word2vec.model')\n",
    "data = pd.read_csv('labelled_news.csv')\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "neu = []\n",
    "for l in data.LABEL:\n",
    "    if l == \"POS\":\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "        neu.append(0)\n",
    "    elif l == \"NEG\":\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "        neu.append(0)\n",
    "    else:\n",
    "        pos.append(0)\n",
    "        neg.append(0)\n",
    "        neu.append(1)\n",
    "data['POS'] = pos\n",
    "data['NEG'] = neg\n",
    "data['NEU'] = neu\n",
    "\n",
    "\n",
    "data['Text_final'] = data['SENTENCE'].apply(lambda x: preprocessing(x))\n",
    "data['tokens'] = data['Text_final'].apply(lambda x: get_tokens(x))\n",
    "print(data[:4])\n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)\n",
    "\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "\n",
    "\n",
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Text_final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_final\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.zeros(EMBEDDING_DIM)\n",
    "# print(train_embedding_weights.shape)\n",
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "label_names = ['POS', 'NEG', 'NEU']\n",
    "y_train = data_train[label_names].values\n",
    "x_train = train_cnn_data\n",
    "y_tr = y_train\n",
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))\n",
    "num_epochs = 15\n",
    "batch_size = 64\n",
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0, shuffle=True, batch_size=batch_size)\n",
    "predictions = model.predict(test_cnn_data, batch_size=64, verbose=1)\n",
    "model.save(\"cnn_model.model\")\n",
    "labels = ['POS','NEG','NEU']\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "print(\"ACCURACY: \", sum(data_test.LABEL==prediction_labels)/len(prediction_labels))\n",
    "print(data_test.LABEL.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
