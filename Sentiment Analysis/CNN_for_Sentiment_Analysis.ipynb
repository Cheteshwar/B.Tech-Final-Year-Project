{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    CNN implementation\n",
    "'''\n",
    "\n",
    "# import statements\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DEFINE CNN\n",
    "'''\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                           SENTENCE LABEL  POS  \\\n",
      "0           0  rec net profit up 8% at rs1,751 crore in july-...   POS    1   \n",
      "1           1        the consequences of declining trust in ceos   POS    1   \n",
      "2           2     mca pulls up companies over lack of csr spends   NEG    0   \n",
      "3           3  demonetization to give major push to e-wallets...   POS    1   \n",
      "\n",
      "   NEG  NEU                                         Text_final  \\\n",
      "0    0    0  rec net profit 8 rs1751 crore julyseptember qu...   \n",
      "1    0    0                  consequences declining trust ceos   \n",
      "2    1    0                mca pulls companies lack csr spends   \n",
      "3    0    0    demonetization major push ewallets payments upi   \n",
      "\n",
      "                                              tokens  \n",
      "0  [rec, net, profit, 8, rs1751, crore, julysepte...  \n",
      "1             [consequences, declining, trust, ceos]  \n",
      "2         [mca, pulls, companies, lack, csr, spends]  \n",
      "3  [demonetization, major, push, ewallets, paymen...  \n",
      "14231 words total, with a vocabulary size of 5071\n",
      "Max sentence length is 13\n",
      "1054 words total, with a vocabulary size of 772\n",
      "Max sentence length is 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdulsmapara/environments/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:76: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/home/abdulsmapara/environments/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:76: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 40, 300)      1521300     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 39, 200)      120200      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 38, 200)      180200      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 37, 200)      240200      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 36, 200)      300200      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 35, 200)      360200      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 200)          0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 200)          0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 200)          0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 200)          0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 200)          0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1000)         0           global_max_pooling1d_11[0][0]    \n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1000)         0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          128128      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            387         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,850,815\n",
      "Trainable params: 1,329,515\n",
      "Non-trainable params: 1,521,300\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    text = nlp(text_nopunct.lower())\n",
    "    final_text = \"\"\n",
    "    for token in text:\n",
    "        if token.is_stop == False:\n",
    "            final_text += token.text + \" \"\n",
    "    return final_text.strip()\n",
    "\n",
    "def get_tokens(text):\n",
    "    text = nlp(text)\n",
    "    tokens = []\n",
    "    for token in text:\n",
    "        tokens.append(token.text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load('custom_word2vec.model')\n",
    "data = pd.read_csv('final_data.csv')\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "neu = []\n",
    "for l in data.LABEL:\n",
    "    if l == \"POS\":\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "        neu.append(0)\n",
    "    elif l == \"NEG\":\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "        neu.append(0)\n",
    "    else:\n",
    "        pos.append(0)\n",
    "        neg.append(0)\n",
    "        neu.append(1)\n",
    "data['POS'] = pos\n",
    "data['NEG'] = neg\n",
    "data['NEU'] = neu\n",
    "\n",
    "\n",
    "data['Text_final'] = data['SENTENCE'].apply(lambda x: preprocessing(x))\n",
    "data['tokens'] = data['Text_final'].apply(lambda x: get_tokens(x))\n",
    "print(data[:4])\n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size=0.07, random_state=42)\n",
    "\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "\n",
    "\n",
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Text_final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_final\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.zeros(EMBEDDING_DIM)\n",
    "# print(train_embedding_weights.shape)\n",
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "label_names = ['POS', 'NEG', 'NEU']\n",
    "y_train = data_train[label_names].values\n",
    "x_train = train_cnn_data\n",
    "y_tr = y_train\n",
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))\n",
    "num_epochs = 15\n",
    "batch_size = 64\n",
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0, shuffle=True, batch_size=batch_size)\n",
    "predictions = model.predict(test_cnn_data, batch_size=64, verbose=1)\n",
    "model.save(\"cnn_model_exp.model\")\n",
    "labels = ['POS','NEG','NEU']\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "print(\"ACCURACY: \", sum(data_test.LABEL==prediction_labels)/len(prediction_labels))\n",
    "print(data_test.LABEL.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
